{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fc1194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows Loaded: 286741\n",
      "Data preprocessing complete! Small files saved to /data/\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# --- 1. Create 'data' folder ---\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# --- 2. Load your original dataset ---\n",
    "file_path = \"data/Food_Inspections.csv\"  # <-- Update if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(f\"Rows Loaded: {len(df)}\")\n",
    "\n",
    "# --- 3. Initial Cleaning ---\n",
    "df[\"Inspection Date\"] = pd.to_datetime(df[\"Inspection Date\"], errors=\"coerce\")\n",
    "df = df[df[\"Inspection Date\"].dt.year >= 2019]\n",
    "df[\"Failed\"] = df[\"Results\"].str.upper().str.contains(\"FAIL\").astype(int)\n",
    "df[\"Month\"] = df[\"Inspection Date\"].dt.to_period(\"M\").astype(str)\n",
    "df[\"Month\"] = pd.to_datetime(df[\"Month\"])\n",
    "df[\"Zip\"] = df[\"Zip\"].astype(str)\n",
    "df[\"Facility Type\"] = df[\"Facility Type\"].fillna(\"UNKNOWN\").str.upper().str.strip()\n",
    "df[\"Risk\"] = df[\"Risk\"].fillna(\"Not Specified\")\n",
    "df[\"Risk\"] = df[\"Risk\"].str.extract(r'(Risk \\d)').fillna(\"Not Specified\")\n",
    "\n",
    "# --- 4. Create each visualization-specific small CSVs ---\n",
    "\n",
    "# 4.1 Monthly Failure Rates\n",
    "monthly_failure = df.groupby(\"Month\")[\"Failed\"].mean().reset_index()\n",
    "monthly_failure[\"Smoothed\"] = monthly_failure[\"Failed\"].rolling(window=3, center=True).mean()\n",
    "monthly_failure.to_csv(\"data/monthly_failure.csv\", index=False)\n",
    "\n",
    "# 4.2 Top 30 ZIP Codes by Failure Rate\n",
    "failure_by_zip = df.groupby(\"Zip\")[\"Failed\"].mean().reset_index()\n",
    "failure_by_zip = failure_by_zip.sort_values(\"Failed\", ascending=False).head(30)\n",
    "failure_by_zip.to_csv(\"data/top30_zip_failure.csv\", index=False)\n",
    "\n",
    "# 4.3 Top 20 Most Common Violation Codes\n",
    "df[\"Violation Codes\"] = df[\"Violations\"].str.findall(r'(\\d+)\\.')\n",
    "all_codes = [code for codes in df[\"Violation Codes\"].dropna() for code in codes]\n",
    "code_counts = pd.DataFrame(Counter(all_codes).items(), columns=[\"Violation Code\", \"Count\"])\n",
    "code_counts = code_counts.sort_values(\"Count\", ascending=False).head(20)\n",
    "code_counts.to_csv(\"data/violation_codes_top20.csv\", index=False)\n",
    "\n",
    "# 4.4 Facility Type vs Inspection Results\n",
    "facility_result_counts = df.groupby([\"Facility Type\", \"Results\"]).size().reset_index(name=\"Count\")\n",
    "facility_result_counts.to_csv(\"data/facility_type_results.csv\", index=False)\n",
    "\n",
    "# 4.5 Monthly Inspection Volume\n",
    "monthly_counts = df.groupby(\"Month\").size().reset_index(name=\"Inspection Count\")\n",
    "monthly_counts.to_csv(\"data/monthly_inspection_volume.csv\", index=False)\n",
    "\n",
    "# 4.6 Risk Level - Inspection Outcomes\n",
    "risk_df = df[df[\"Risk\"].isin([\"Risk 1\", \"Risk 2\", \"Risk 3\"])].copy()\n",
    "risk_df.to_csv(\"data/risk_level_failure.csv\", index=False)\n",
    "\n",
    "# 4.7 Time Series for Brushing (Date vs Failed)\n",
    "df_time_series = df[[\"Inspection Date\", \"Failed\"]].dropna()\n",
    "df_time_series.to_csv(\"data/time_series_failure.csv\", index=False)\n",
    "\n",
    "print(\"Data preprocessing complete! Small files saved to /data/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd94c64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved monthly_facility_counts.csv!\n"
     ]
    }
   ],
   "source": [
    "# --- New Aggregation: Inspections per Month + Facility Type ---\n",
    "monthly_facility_counts = (\n",
    "    df.groupby([\"Month\", \"Facility Type\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Inspection Count\")\n",
    ")\n",
    "\n",
    "# Save it\n",
    "monthly_facility_counts.to_csv(\"data/monthly_facility_counts.csv\", index=False)\n",
    "\n",
    "print(\"Saved monthly_facility_counts.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74894b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Make a copy ---\n",
    "df_copy = df.copy()\n",
    "\n",
    "# --- Step 2: Filter Risk levels only ---\n",
    "df_copy = df_copy[df_copy[\"Risk\"].isin([\"Risk 1\", \"Risk 2\", \"Risk 3\"])]\n",
    "\n",
    "# --- Step 3: Explode Violations ---\n",
    "df_copy[\"Violation Codes\"] = df_copy[\"Violations\"].str.findall(r'(\\d+)\\.')\n",
    "df_exploded = df_copy.explode(\"Violation Codes\")\n",
    "df_exploded[\"Violation Codes\"] = df_exploded[\"Violation Codes\"].astype(str)\n",
    "\n",
    "# --- Step 4: Group by Risk + Violation Code ---\n",
    "violation_by_risk = (\n",
    "    df_exploded.groupby([\"Risk\", \"Violation Codes\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "# --- Step 5: Save the new file ---\n",
    "violation_by_risk.to_csv('data/violation_by_risk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5667b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. For Facility and Risk Views ---\n",
    "df_facility = df.copy()\n",
    "df_facility[\"Facility Type\"] = df_facility[\"Facility Type\"].fillna(\"UNKNOWN\").str.upper().str.strip()\n",
    "\n",
    "# Save inspection_data.csv\n",
    "df_facility_small = df_facility[[\"Inspection Date\", \"Facility Type\", \"Failed\", \"Risk\", \"Violations\"]]\n",
    "df_facility_small.to_csv(\"data/inspection_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46a63a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final clean inspection_data_for_zip.csv saved with 104461 rows!\n"
     ]
    }
   ],
   "source": [
    "# Parse dates\n",
    "df[\"Inspection Date\"] = pd.to_datetime(df[\"Inspection Date\"], errors=\"coerce\")\n",
    "\n",
    "# Extract year\n",
    "df[\"Inspection Year\"] = df[\"Inspection Date\"].dt.year\n",
    "\n",
    "# Filter years 2019â€“2024\n",
    "df = df[df[\"Inspection Year\"].isin([2019, 2020, 2021, 2022, 2023, 2024])]\n",
    "\n",
    "# Create Failed column\n",
    "df[\"Failed\"] = df[\"Results\"].str.upper().str.contains(\"FAIL\").astype(int)\n",
    "\n",
    "# Clean Zip\n",
    "df[\"Zip\"] = df[\"Zip\"].fillna(\"\").astype(str).str.strip()\n",
    "df[\"Zip\"] = df[\"Zip\"].str.extract(r'(\\d{5})')\n",
    "\n",
    "# Drop NaN Zips\n",
    "df = df[df[\"Zip\"].notna()]\n",
    "\n",
    "# Final minimal dataset\n",
    "df_zip_small = df[[\"Inspection Date\", \"Zip\", \"Failed\"]]\n",
    "\n",
    "# Save\n",
    "df_zip_small.to_csv('data/inspection_data_for_zip.csv', index=False)\n",
    "\n",
    "print(f\"Final clean inspection_data_for_zip.csv saved with {len(df_zip_small)} rows!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283eae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load ZIP Code GeoJSON ===\n",
    "geo_path = \"data/Boundaries - ZIP Codes_20250402.geojson\"\n",
    "geo = gpd.read_file(geo_path)\n",
    "\n",
    "# Ensure ZIP code is a zero-padded string\n",
    "geo[\"zip\"] = geo[\"zip\"].astype(str).str.zfill(5)\n",
    "\n",
    "# Keep only needed columns\n",
    "geo = geo[[\"zip\", \"geometry\"]]\n",
    "\n",
    "# === Step 2: Load Food Inspection CSV ===\n",
    "df = pd.read_csv(\"data/Food_Inspections.csv\")\n",
    "\n",
    "# Parse dates and filter to 2019+\n",
    "df[\"Inspection Date\"] = pd.to_datetime(df[\"Inspection Date\"], errors=\"coerce\")\n",
    "df = df[df[\"Inspection Date\"].dt.year >= 2019]\n",
    "\n",
    "# Clean up result column\n",
    "df[\"Results\"] = df[\"Results\"].str.upper().str.strip()\n",
    "df[\"Failed\"] = df[\"Results\"].str.contains(\"FAIL\", na=False).astype(int)\n",
    "\n",
    "# Normalize ZIP column\n",
    "df[\"Zip\"] = df[\"Zip\"].astype(str).str.split('.').str[0].str.zfill(5)\n",
    "\n",
    "# === Step 3: Calculate failure rate per ZIP ===\n",
    "fail_rate = df.groupby(\"Zip\")[\"Failed\"].mean().reset_index()\n",
    "fail_rate.columns = [\"zip\", \"FailureRate\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c8af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload dataset after kernel reset\n",
    "df = pd.read_csv(\"data/Food_Inspections.csv\")\n",
    "\n",
    "# Convert 'Inspection Date' to datetime\n",
    "df['Inspection Date'] = pd.to_datetime(df['Inspection Date'], errors='coerce')\n",
    "\n",
    "# Filter only valid risk levels\n",
    "valid_risks = ['Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)']\n",
    "df = df[df['Risk'].isin(valid_risks)].copy()\n",
    "\n",
    "# Create 'Month' as quarter-based period\n",
    "df['Month'] = df['Inspection Date'].dt.to_period('Q').astype(str)\n",
    "\n",
    "# Optional: enforce ordered categories for Risk to fix Y-axis order in heatmap\n",
    "df['Risk'] = pd.Categorical(df['Risk'], categories=valid_risks, ordered=True)\n",
    "\n",
    "# build heatmap data\n",
    "heatmap_data = (\n",
    "    df.groupby(['Month', 'Risk'], observed=True)\n",
    "    .agg(\n",
    "        total_inspections=('Results', 'count'),\n",
    "        failed_inspections=('Results', lambda x: (x == 'Fail').sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "heatmap_data['Failure Rate'] = (\n",
    "    heatmap_data['failed_inspections'] / heatmap_data['total_inspections'] * 100\n",
    ").round(2)\n",
    "\n",
    "# Save cleaned heatmap data\n",
    "heatmap_data.to_csv(\"data/heatmap_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Violin Plot Data: Add Month + Risk + Violation Count\n",
    "# -----------------------------\n",
    "# Create 'Violation Count' from Violations column\n",
    "df['Violation Count'] = df['Violations'].fillna('').apply(lambda x: len(x.split('|')) if x else 0)\n",
    "\n",
    "# Keep only rows with non-null Inspection Type\n",
    "df_violin = df[['Inspection Type', 'Facility Type', 'Violation Count', 'Month']].dropna()\n",
    "\n",
    "# Save the new violin dataset\n",
    "df_violin.to_csv(\"data/violin_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "580aedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original Food Inspections dataset\n",
    "df = pd.read_csv(\"data/Food_Inspections.csv\")\n",
    "\n",
    "# Ensure 'Inspection Date' is in datetime format\n",
    "df['Inspection Date'] = pd.to_datetime(df['Inspection Date'], errors='coerce')\n",
    "\n",
    "# Keep only valid Risk levels\n",
    "df = df[df['Risk'].isin(['Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)'])].copy()\n",
    "\n",
    "# Add 'Month' column as Year-Quarter string\n",
    "df['Month'] = df['Inspection Date'].dt.to_period('Q').astype(str)\n",
    "\n",
    "# Create grouped summary for treemap\n",
    "treemap_data = (\n",
    "    df.groupby(['Month', 'Risk', 'Facility Type'])\n",
    "    .agg(\n",
    "        total_inspections=('Results', 'count'),\n",
    "        failed_inspections=('Results', lambda x: (x == 'Fail').sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add new column with custom name to avoid overwriting\n",
    "treemap_data[\"Treemap Failure Rate\"] = (\n",
    "    treemap_data[\"failed_inspections\"] / treemap_data[\"total_inspections\"] * 100\n",
    ").round(2)\n",
    "\n",
    "# Save to a new file\n",
    "treemap_data.to_csv(\"data/treemap_data_custom.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32a0c194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported 'minimized_choropleth.json' with essential fields.\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Load ZIP Code GeoJSON ===\n",
    "geo_path = \"data/Boundaries - ZIP Codes_20250402.geojson\"\n",
    "geo = gpd.read_file(geo_path)\n",
    "\n",
    "# Ensure ZIP code is a zero-padded string\n",
    "geo[\"zip\"] = geo[\"zip\"].astype(str).str.zfill(5)\n",
    "\n",
    "# Keep only needed columns\n",
    "geo = geo[[\"zip\", \"geometry\"]]\n",
    "\n",
    "# === Step 2: Load Food Inspection CSV ===\n",
    "df = pd.read_csv(\"data/Food_Inspections.csv\")\n",
    "\n",
    "# Parse dates and filter to 2019+\n",
    "df[\"Inspection Date\"] = pd.to_datetime(df[\"Inspection Date\"], errors=\"coerce\")\n",
    "df = df[df[\"Inspection Date\"].dt.year >= 2019]\n",
    "\n",
    "# Clean up result column\n",
    "df[\"Results\"] = df[\"Results\"].str.upper().str.strip()\n",
    "df[\"Failed\"] = df[\"Results\"].str.contains(\"FAIL\", na=False).astype(int)\n",
    "\n",
    "# Normalize ZIP column\n",
    "df[\"Zip\"] = df[\"Zip\"].astype(str).str.split('.').str[0].str.zfill(5)\n",
    "\n",
    "# === Step 3: Calculate failure rate per ZIP ===\n",
    "fail_rate = df.groupby(\"Zip\")[\"Failed\"].mean().reset_index()\n",
    "fail_rate.columns = [\"zip\", \"FailureRate\"]\n",
    "\n",
    "# === Step 4: Merge with GeoJSON ===\n",
    "merged = geo.merge(fail_rate, on=\"zip\", how=\"left\")\n",
    "\n",
    "# === Step 5: Reproject to EPSG:4326 (Web Mercator) ===\n",
    "if merged.crs is None or merged.crs.to_string() != 'EPSG:4326':\n",
    "    merged = merged.to_crs(epsg=4326)\n",
    "\n",
    "# === Step 6: Export minimal GeoJSON ===\n",
    "merged[[\"zip\", \"geometry\", \"FailureRate\"]].to_file(\"data/minimized_choropleth.json\", driver=\"GeoJSON\")\n",
    "\n",
    "print(\"âœ… Exported 'minimized_choropleth.json' with essential fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ae9fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported simplified chicago_boundary.json\n"
     ]
    }
   ],
   "source": [
    "# Load ZIP GeoJSON\n",
    "zip_gdf = gpd.read_file(\"data/Boundaries - ZIP Codes_20250402.geojson\")\n",
    "\n",
    "# âœ… Dissolve all shapes into one â€” ignore 'zip', 'objectid', etc.\n",
    "city_boundary = zip_gdf.dissolve()\n",
    "\n",
    "# âœ… Reset geometry CRS to EPSG:4326 for web maps\n",
    "city_boundary = city_boundary.to_crs(epsg=4326)\n",
    "\n",
    "# âœ… Export the true boundary (will now be 1 shape, no zip)\n",
    "city_boundary.to_file(\"data/chicago_boundary.json\", driver=\"GeoJSON\")\n",
    "\n",
    "print(\"âœ… Exported simplified chicago_boundary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bc14f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fixed and saved: data/score_violation_parallel.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Use only required columns and drop rows with missing data\n",
    "df_clean = df[['Facility Type', 'Results', 'Violations']].dropna()\n",
    "\n",
    "# Map results to numeric scores (proxy for inspection score)\n",
    "result_map = {\n",
    "    'Pass': 2,\n",
    "    'Pass w/ Conditions': 1,\n",
    "    'Fail': 0\n",
    "}\n",
    "df_clean['Inspection Score'] = df_clean['Results'].map(result_map)\n",
    "\n",
    "# Drop rows with unmapped results (like 'Out of Business')\n",
    "df_clean = df_clean.dropna(subset=['Inspection Score'])\n",
    "\n",
    "# Improved violation counter using regex (counts patterns like \"10. \" or \"5. \")\n",
    "def count_violations(v):\n",
    "    if isinstance(v, str):\n",
    "        return len(re.findall(r'\\d+\\.', v))\n",
    "    return 1\n",
    "\n",
    "df_clean['Violation Count'] = df_clean['Violations'].apply(count_violations)\n",
    "\n",
    "\n",
    "# âœ… Violation severity scoring\n",
    "severity_keywords = {\n",
    "    'rodent': 3,\n",
    "    'pest': 2,\n",
    "    'sewage': 4,\n",
    "    'contamination': 3,\n",
    "    'temperature': 2,\n",
    "    'cleaning': 1,\n",
    "    'food': 1,\n",
    "    'hygiene': 2,\n",
    "    'handwashing': 2\n",
    "}\n",
    "\n",
    "def compute_severity(violation_text):\n",
    "    text = str(violation_text).lower()\n",
    "    return sum(weight for word, weight in severity_keywords.items() if word in text)\n",
    "\n",
    "df_clean['Violation Severity Score'] = df_clean['Violations'].apply(compute_severity)\n",
    "\n",
    "# âœ… Limit to top 30 facility types\n",
    "top_30 = df_clean['Facility Type'].value_counts().nlargest(30).index\n",
    "df_filtered = df_clean[df_clean['Facility Type'].isin(top_30)]\n",
    "\n",
    "# âœ… Group and aggregate raw values\n",
    "df_grouped = (\n",
    "    df_filtered.groupby('Facility Type')\n",
    "    .agg({\n",
    "        'Inspection Score': 'mean',\n",
    "        'Violation Count': 'mean',\n",
    "        'Violation Severity Score': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# âœ… Normalize after aggregation\n",
    "scaler = MinMaxScaler()\n",
    "df_grouped[['Score_norm', 'Count_norm', 'Severity_norm']] = scaler.fit_transform(\n",
    "    df_grouped[['Inspection Score', 'Violation Count', 'Violation Severity Score']]\n",
    ")\n",
    "\n",
    "# âœ… Save to file\n",
    "df_grouped.to_csv('data/score_violation_parallel.csv', index=False)\n",
    "print(\"âœ… Fixed and saved: data/score_violation_parallel.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb9b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bubble = (\n",
    "    df_viz\n",
    "    .groupby('Facility Type')\n",
    "    .agg({\n",
    "        'Violation Count': 'mean',\n",
    "        'Violation Severity Score': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'Violation Count': 'Avg Violation Count',\n",
    "        'Violation Severity Score': 'Avg Severity Score'\n",
    "    })\n",
    ")\n",
    "\n",
    "# Optional: Keep top 50 largest bubbles\n",
    "df_bubble = df_bubble.nlargest(50, 'Avg Violation Count')\n",
    "\n",
    "df_bubble.to_csv('data/packed_bubble_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a81cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
